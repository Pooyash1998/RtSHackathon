"""
panel_review.py

Multimodal quality check for generated comic panels.

Given:
  - image_url (from FLUX / BFL sample URL)
  - panel script (narration, dialogue, featured students)
  - classroom + students context

It:
  - Uses an OpenAI vision-capable model to inspect the image
  - Compares it against the expected text and characters
  - Returns a JSON score (0‚Äì10) plus concrete issues and a suggested fix prompt.
"""

import os
import json
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "YOUR_OPENAI_API_KEY_HERE")
# Separate model for QA so you can tweak independently
OPENAI_QA_MODEL = os.getenv("OPENAI_QA_MODEL", "gpt-4o")

openai_client = OpenAI(api_key=OPENAI_API_KEY)


def _expected_text_from_panel(panel: Dict[str, Any]) -> List[Dict[str, str]]:
    """
    Represent narration + dialogue as a structured list so the model can
    judge text accuracy more easily.
    """
    out: List[Dict[str, str]] = []

    narration = (panel.get("narration") or "").strip()
    if narration:
        out.append({"type": "narration", "text": narration})

    for line in panel.get("dialogue") or []:
        speaker = (line.get("speaker") or "").strip()
        text = (line.get("text") or "").strip()
        if not text:
            continue
        out.append(
            {
                "type": "dialogue",
                "speaker": speaker,
                "text": text,
            }
        )

    return out


def review_panel_image(
    image_url: str,
    panel: Dict[str, Any],
    classroom: Dict[str, Any],
    students: List[Dict[str, Any]],
    min_score: float = 8.0,
) -> Dict[str, Any]:
    """
    Ask a multimodal OpenAI model to review a single comic panel image.

    Returns a dict like:
    {
      "score": 8.7,
      "dimensions": {
        "text_accuracy": 9.0,
        "character_accuracy": 8.0,
        "layout_readability": 9.0
      },
      "issues": [
        "Speech bubble for LENA is missing",
        "Text 'F=ma' is misspelled"
      ],
      "suggested_fix_prompt": "Add a speech bubble for LENA saying '...' and correct 'F=ma' text.",
      "notes": "Additional free-form comments if needed."
    }
    """
    
    print(f"         üîç Starting quality review...")
    print(f"         ‚Üí Image URL: {image_url[:60]}...")

    if not OPENAI_API_KEY or OPENAI_API_KEY == "YOUR_OPENAI_API_KEY_HERE":
        raise RuntimeError("OPENAI_API_KEY not set; cannot run panel review")

    expected_text = _expected_text_from_panel(panel)
    featured_students = panel.get("featured_students") or []
    setting = (panel.get("setting") or "").strip()
    description = (panel.get("description") or "").strip()

    # Build a compact map of student info for the reviewer
    students_by_name: Dict[str, Dict[str, Any]] = {
        s["name"]: s for s in students if "name" in s
    }

    student_info = []
    for name in featured_students:
        s = students_by_name.get(name)
        if not s:
            continue
        student_info.append(
            {
                "name": s["name"],
                "interests": s.get("interests", ""),
                "avatar_url": s.get("avatar_url"),
            }
        )

    review_payload = {
        "panel_index": panel.get("index"),
        "expected_setting": setting,
        "expected_visual_description": description,
        "expected_text": expected_text,
        "expected_featured_students": featured_students,
        "classroom_subject": classroom.get("subject"),
        "classroom_grade": classroom.get("grade_level"),
        "classroom_story_theme": classroom.get("story_theme"),
        "student_info": student_info,
        "target_min_score": min_score,
    }

    system_prompt = (
        "You are a strict, zero-tolerance art director for kid-friendly educational comic books.\n"
        "You will be given the expected script for ONE panel (narration, dialogue, "
        "featured student names) plus the rendered image of that panel.\n"
        "Your job is to rate how well the image matches the script and suggest simple "
        "prompt-level fixes so the panel can be regenerated by a text-to-image model.\n\n"
        "Pay EXTREME attention to speech bubbles and text correctness:\n"
        "- Each bubble's tail MUST clearly point to the correct speaker.\n"
        "- No character may speak lines that belong to someone else.\n"
        "- Characters must not speak about themselves in an unnatural way "
        "(for example, referring to themselves in the third person or addressing "
        "themselves by name) unless the script explicitly requires it.\n"
        "- Text in narration and bubbles MUST match the expected text EXACTLY.\n"
        "- ANY spelling error, garbled word, missing word, extra word, or paraphrasing "
        "counts as a serious error.\n\n"
        "Scoring rules for each dimension (0‚Äì10):\n"
        "- text_accuracy:\n"
        "  * 10 = every word in narration and speech bubbles is perfectly legible and "
        "    exactly matches the expected text (no differences at all).\n"
        "  * 5‚Äì9 = reserved ONLY for very minor visual imperfections (e.g. slightly odd "
        "    font rendering) while the text content still matches exactly.\n"
        "  * 0‚Äì3 = ANY mistake in content (spelling differences, missing or extra words, "
        "    nonsense text, garbled text, or any line belonging to the wrong speaker). "
        "    If you see even a single such issue, text_accuracy MUST be 3 or lower.\n"
        "- character_accuracy:\n"
        "  * 10 = all named students that should appear are clearly present, not merged, "
        "    consistent with each other, and the bubbles attached to them are correct.\n"
        "  * 5‚Äì9 = only very minor stylistic deviations, but the cast and bubble "
        "    assignments are still clearly correct.\n"
        "  * 0‚Äì3 = any missing required character, wrong character speaking a line, or "
        "    bubble tails pointing at the wrong person. If you see any such issue, "
        "    character_accuracy MUST be 3 or lower.\n"
        "- layout_readability:\n"
        "  * 10 = composition is clean, speech bubbles / narration boxes do not overlap "
        "    faces, text is not cropped, and the panel is easy to read for kids.\n"
        "  * Lower scores for clutter, overlapping text, tiny text, or cropped bubbles.\n\n"
        "Overall 'score' must be a weighted average with MUCH more weight on "
        "text_accuracy and character_accuracy than layout_readability.\n"
        "If there is ANY text mistake, garbled text, or misaligned bubble (bubble tail "
        "clearly pointing to the wrong character), the overall score MUST NOT exceed 5, "
        "and can be as low as 0 for very poor matches.\n"
    )

    print(f"         ‚Üí Building review prompt...")
    print(f"         ‚Üí Expected students: {featured_students}")
    print(f"         ‚Üí Expected text lines: {len(expected_text)}")
    
    user_prompt_text = (
        "Here is the structured description of what this panel SHOULD contain.\n"
        "Then you see the actual rendered image.\n\n"
        "Tasks:\n"
        "1) Carefully compare the text in the image (narration, bubbles, labels) to the "
        "expected text. Note any missing, added, paraphrased, or unreadable words.\n"
        "2) Check speech-bubble alignment:\n"
        "   - For each dialogue line, is there a bubble whose tail clearly points to the "
        "     correct character?\n"
        "   - Are there any bubbles pointing to the wrong character, or characters speaking "
        "     even though they have no dialogue defined for this panel?\n"
        "   - Does any character talk in an obviously wrong way, such as referring to "
        "     themselves in the third person or addressing themselves by name?\n"
        "3) Check whether all named students that are supposed to appear are clearly present "
        "   and consistent with their roles.\n"
        "4) Check whether the layout makes the text easy to read (no cropping, no weird "
        "   overlaps, text big enough for kids).\n\n"
        "Then respond ONLY with a single JSON object with this structure:\n"
        "{\n"
        '  "score": float,                 // 0‚Äì10 overall\n'
        '  "dimensions": {\n'
        '    "text_accuracy": float,       // 0‚Äì10 (include correctness of text AND who says it)\n'
        '    "character_accuracy": float,  // 0‚Äì10 (include whether bubbles attach to correct characters)\n'
        '    "layout_readability": float   // 0‚Äì10\n'
        "  },\n"
        '  "issues": [ "string", ... ],    // list of concrete problems (mention bubble misalignment explicitly)\n'
        '  "suggested_fix_prompt": "short text prompt to append to the image model prompt, '
        "max 2‚Äì3 sentences, focusing on the most important fixes such as moving specific "
        "bubbles to the right character, correcting mis-written text, or adding/removing "
        'bubbles.",\n'
        '  "notes": "optional extra comments or explanations"\n'
        "}\n\n"
        "Be concise and practical in 'issues' and 'suggested_fix_prompt'. For example, you "
        "might say: \"Move the bubble with 'F = ma' so the tail points to LENA on the left; "
        "replace the current text with 'F = m ¬∑ a'; remove the extra bubble above the teacher.\"\n"
        "If everything already looks perfect, you can still report a high score (e.g. 9.5‚Äì10) "
        "and leave 'issues' empty and 'suggested_fix_prompt' as an empty string.\n\n"
        f"PANEL SPEC JSON:\n{json.dumps(review_payload, ensure_ascii=False)}"
    )

    print(f"         ‚Üí Calling OpenAI Vision API ({OPENAI_QA_MODEL})...")
    
    resp = openai_client.chat.completions.create(
        model=OPENAI_QA_MODEL,
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": system_prompt},
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": user_prompt_text},
                    {
                        "type": "image_url",
                        "image_url": {"url": image_url},
                    },
                ],
            },
        ],
    )

    print(f"         ‚Üí Received response, parsing...")
    raw = resp.choices[0].message.content
    try:
        data = json.loads(raw)
    except json.JSONDecodeError as e:
        raise RuntimeError(f"Panel review returned invalid JSON: {e}\nRaw: {raw}")

    # Light normalization so caller can assume keys exist
    data.setdefault("score", 0.0)
    dims = data.setdefault("dimensions", {})
    dims.setdefault("text_accuracy", 0.0)
    dims.setdefault("character_accuracy", 0.0)
    dims.setdefault("layout_readability", 0.0)
    data.setdefault("issues", [])
    data.setdefault("suggested_fix_prompt", "")
    data.setdefault("notes", "")

    print(f"         ‚úì Review complete!")
    print(f"         ‚Üí Overall score: {data['score']:.1f}/10")
    print(f"         ‚Üí Text accuracy: {dims['text_accuracy']:.1f}/10")
    print(f"         ‚Üí Character accuracy: {dims['character_accuracy']:.1f}/10")
    print(f"         ‚Üí Layout readability: {dims['layout_readability']:.1f}/10")
    if data['issues']:
        print(f"         ‚Üí Issues found: {len(data['issues'])}")

    return data
