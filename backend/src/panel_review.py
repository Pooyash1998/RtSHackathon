"""
panel_review.py

Multimodal quality check for generated comic panels.

Given:
  - image_url (from FLUX / BFL sample URL)
  - panel script (narration, dialogue, featured students)
  - classroom + students context

It:
  - Uses an OpenAI vision-capable model to inspect the image
  - Compares it against the expected text and characters
  - Returns a JSON score (0–10) plus concrete issues and a suggested fix prompt.
"""

import os
import json
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "YOUR_OPENAI_API_KEY_HERE")
# Separate model for QA so you can tweak independently
OPENAI_QA_MODEL = os.getenv("OPENAI_QA_MODEL", "gpt-4o")

openai_client = OpenAI(api_key=OPENAI_API_KEY)


def _expected_text_from_panel(panel: Dict[str, Any]) -> List[Dict[str, str]]:
    """
    Represent narration + dialogue as a structured list so the model can
    judge text accuracy more easily.
    """
    out: List[Dict[str, str]] = []

    narration = (panel.get("narration") or "").strip()
    if narration:
        out.append({"type": "narration", "text": narration})

    for line in panel.get("dialogue") or []:
        speaker = (line.get("speaker") or "").strip()
        text = (line.get("text") or "").strip()
        if not text:
            continue
        out.append(
            {
                "type": "dialogue",
                "speaker": speaker,
                "text": text,
            }
        )

    return out


def review_panel_image(
    image_url: str,
    panel: Dict[str, Any],
    classroom: Dict[str, Any],
    students: List[Dict[str, Any]],
    min_score: float = 8.0,
) -> Dict[str, Any]:
    """
    Ask a multimodal OpenAI model to review a single comic panel image.

    Returns a dict like:
    {
      "score": 8.7,
      "dimensions": {
        "text_accuracy": 9.0,
        "character_accuracy": 8.0,
        "layout_readability": 9.0
      },
      "issues": [
        "Speech bubble for LENA is missing",
        "Text 'F=ma' is misspelled"
      ],
      "suggested_fix_prompt": "Add a speech bubble for LENA saying '...' and correct 'F=ma' text.",
      "notes": "Additional free-form comments if needed."
    }
    """

    if not OPENAI_API_KEY or OPENAI_API_KEY == "YOUR_OPENAI_API_KEY_HERE":
        raise RuntimeError("OPENAI_API_KEY not set; cannot run panel review")

    expected_text = _expected_text_from_panel(panel)
    featured_students = panel.get("featured_students") or []
    setting = (panel.get("setting") or "").strip()
    description = (panel.get("description") or "").strip()

    # Build a compact map of student info for the reviewer
    students_by_name: Dict[str, Dict[str, Any]] = {
        s["name"]: s for s in students if "name" in s
    }

    student_info = []
    for name in featured_students:
        s = students_by_name.get(name)
        if not s:
            continue
        student_info.append(
            {
                "name": s["name"],
                "interests": s.get("interests", ""),
                "avatar_url": s.get("avatar_url"),
            }
        )

    review_payload = {
        "panel_index": panel.get("index"),
        "expected_setting": setting,
        "expected_visual_description": description,
        "expected_text": expected_text,
        "expected_featured_students": featured_students,
        "classroom_subject": classroom.get("subject"),
        "classroom_grade": classroom.get("grade_level"),
        "classroom_story_theme": classroom.get("story_theme"),
        "student_info": student_info,
        "target_min_score": min_score,
    }

system_prompt = (
        "You are a strict art director for kid-friendly educational comic books.\n"
        "You will be given the expected script for ONE panel (narration, dialogue, "
        "featured student names) plus the rendered image of that panel.\n"
        "Your job is to rate how well the image matches the script and suggest simple "
        "prompt-level fixes so the panel can be regenerated by a text-to-image model.\n\n"
        "Pay special attention to speech bubbles:\n"
        "- Each bubble's tail should clearly point to the correct speaker.\n"
        "- No character should speak lines that obviously belong to someone else.\n"
        "- Characters should not speak about themselves in an unnatural way "
        "(for example, referring to themselves in the third person or addressing "
        "themselves by name).\n\n"
        "Scoring rules for each dimension (0–10):\n"
        "- text_accuracy: 10 = all words in narration and speech bubbles are legible and "
        "match the expected text exactly or with tiny differences (capitalization or 1–2 "
        "character spelling mistakes). 8–9 = small differences but clearly the same words. "
        "5–7 = noticeable mistakes, missing lines, or obviously wrong speaker assignments "
        "for the text. 0–4 = text is mostly wrong, unreadable, nonsense, or missing.\n"
        "- character_accuracy: 10 = all named students that should appear are clearly present, "
        "not merged, and look like distinct consistent characters. 8–9 = small deviations but "
        "clearly the right kids. 5–7 = some characters missing or obviously wrong, or speech "
        "bubbles mainly attached to the wrong characters. 0–4 = the cast doesn't match at all.\n"
        "- layout_readability: 10 = composition is clean, speech bubbles / narration boxes do "
        "not overlap faces, text is not cropped, panel is easy to read for kids. Lower scores "
        "for clutter, overlapping text, tiny text, or cropped bubbles.\n\n"
        "Overall 'score' should be a weighted average with more weight on text_accuracy and "
        "character_accuracy than layout_readability. Bubble-to-speaker alignment and unnatural "
        "self-speech should significantly lower text_accuracy and/or character_accuracy."
    )


  user_prompt_text = (
        "Here is the structured description of what this panel SHOULD contain.\n"
        "Then you see the actual rendered image.\n\n"
        "Tasks:\n"
        "1) Carefully compare the text in the image (narration, bubbles, labels) to the "
        "expected text. Note any missing, added, paraphrased, or unreadable words.\n"
        "2) Check speech-bubble alignment:\n"
        "   - For each dialogue line, is there a bubble whose tail clearly points to the "
        "     correct character?\n"
        "   - Are there any bubbles pointing to the wrong character, or characters speaking "
        "     even though they have no dialogue defined for this panel?\n"
        "   - Does any character talk in an obviously wrong way, such as referring to "
        "     themselves in the third person or addressing themselves by name?\n"
        "3) Check whether all named students that are supposed to appear are clearly present "
        "   and consistent with their roles.\n"
        "4) Check whether the layout makes the text easy to read (no cropping, no weird "
        "   overlaps, text big enough for kids).\n\n"
        "Then respond ONLY with a single JSON object with this structure:\n"
        "{\n"
        '  "score": float,                 // 0–10 overall\n'
        '  "dimensions": {\n'
        '    "text_accuracy": float,       // 0–10 (include correctness of text AND who says it)\n'
        '    "character_accuracy": float,  // 0–10 (include whether bubbles attach to correct characters)\n'
        '    "layout_readability": float   // 0–10\n'
        "  },\n"
        '  "issues": [ "string", ... ],    // list of concrete problems (mention bubble misalignment explicitly)\n'
        '  "suggested_fix_prompt": "short text prompt to append to the image model prompt, '
        'max 2–3 sentences, focusing on the most important fixes such as moving specific '
        'bubbles to the right character, correcting mis-written text, or adding/removing '
        'bubbles.",\n'
        '  "notes": "optional extra comments or explanations"\n'
        "}\n\n"
        "Be concise and practical in 'issues' and 'suggested_fix_prompt'. For example, you "
        "might say: \"Move the bubble with 'F = ma' so the tail points to LENA on the left; "
        "replace the current text with 'F = m · a'; remove the extra bubble above the teacher.\"\n"
        "If everything already looks perfect, you can still report a high score (e.g. 9.5–10) "
        "and leave 'issues' empty and 'suggested_fix_prompt' as an empty string.\n\n"
        f"PANEL SPEC JSON:\n{json.dumps(review_payload, ensure_ascii=False)}"
    )

    resp = openai_client.chat.completions.create(
        model=OPENAI_QA_MODEL,
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": system_prompt},
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": user_prompt_text},
                    {
                        "type": "image_url",
                        "image_url": {"url": image_url},
                    },
                ],
            },
        ],
    )

    raw = resp.choices[0].message.content
    try:
        data = json.loads(raw)
    except json.JSONDecodeError as e:
        raise RuntimeError(f"Panel review returned invalid JSON: {e}\nRaw: {raw}")

    # Light normalization so caller can assume keys exist
    data.setdefault("score", 0.0)
    dims = data.setdefault("dimensions", {})
    dims.setdefault("text_accuracy", 0.0)
    dims.setdefault("character_accuracy", 0.0)
    dims.setdefault("layout_readability", 0.0)
    data.setdefault("issues", [])
    data.setdefault("suggested_fix_prompt", "")
    data.setdefault("notes", "")

    return data
